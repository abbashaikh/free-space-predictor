{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Debug train.py via torch.distributed.run",
      "type": "debugpy",
      "request": "launch",

      // use PyTorch’s distributed launcher as a module
      "module": "torch.distributed.run",
      "args": [
        "--nproc_per_node", "1",
        "--master_port", "29500",
        "${workspaceFolder}/train.py",
        "--eval_every", "1",
        "--vis_every", "1",
        "--batch_size", "4",
        "--eval_batch_size", "4",
        "--preprocess_workers", "16",
        "--log_dir", "./data/trained_models/trajectory_prediction",
        "--train_epochs", "15",
        "--conf", "./config/pedestrians.json",
        "--trajdata_cache_dir", "./data/pedestrian_datasets/.unified_data_cache",
        "--train_data", "eth-train",
        "--eval_data", "eth-val",
        "--history_sec", "2.8",
        "--prediction_sec", "4.8",
        "--K", "1",
        "--alpha_init", "1.54e-05",
        "--augment_input_noise", "0.57",
        "--grad_clip", "0.909",
        "--learning_rate", "0.016",
        "--sigma_eps_init", "0.0002",
        "--contrastive_weight", "50.0"
      ],

      // so you can see logs & interact if needed
      "console": "integratedTerminal",
      "cwd": "${workspaceFolder}",

      // for single‐GPU debugging, torch.distributed.run will set LOCAL_RANK=0
      // but if you need to force it, you can uncomment:
      // "env": { "LOCAL_RANK": "0", "MASTER_ADDR": "127.0.0.1" }
    }
  ]
}
